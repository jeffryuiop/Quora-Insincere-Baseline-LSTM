{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T15:44:00.922116Z",
     "start_time": "2018-11-14T15:43:59.609982Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T15:44:01.765164Z",
     "start_time": "2018-11-14T15:44:00.923482Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/job:localhost/replica:0/task:0/device:GPU:0',\n",
       " '/job:localhost/replica:0/task:0/device:GPU:1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "os.environ['cuda_visible_device'] = '0'\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T15:44:03.776791Z",
     "start_time": "2018-11-14T15:44:01.766452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (56370, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "print(\"Train shape : \",train_df.shape)\n",
    "print(\"Test shape : \",test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T15:44:51.070074Z",
     "start_time": "2018-11-14T15:44:03.778151Z"
    }
   },
   "outputs": [],
   "source": [
    "## split to train and val\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.08, random_state=2018)\n",
    "\n",
    "## some config values \n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 95000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 70 # max number of words in a question to use\n",
    "\n",
    "## fill up the missing values\n",
    "train_X = train_df[\"question_text\"].fillna(\"_##_\").values\n",
    "val_X = val_df[\"question_text\"].fillna(\"_##_\").values\n",
    "test_X = test_df[\"question_text\"].fillna(\"_##_\").values\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "val_X = tokenizer.texts_to_sequences(val_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "## Pad the sentences \n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "## Get the target values\n",
    "train_y = train_df['target'].values\n",
    "val_y = val_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T15:44:51.337525Z",
     "start_time": "2018-11-14T15:44:51.071334Z"
    }
   },
   "outputs": [],
   "source": [
    "#shuffling the data\n",
    "np.random.seed(2018)\n",
    "trn_idx = np.random.permutation(len(train_X))\n",
    "val_idx = np.random.permutation(len(val_X))\n",
    "\n",
    "train_X = train_X[trn_idx]\n",
    "val_X = val_X[val_idx]\n",
    "train_y = train_y[trn_idx]\n",
    "val_y = val_y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T15:47:05.889706Z",
     "start_time": "2018-11-14T15:44:51.338869Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'glove.840B.300d/glove.840B.300d.txt'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T15:47:06.297201Z",
     "start_time": "2018-11-14T15:47:05.891079Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/yekenot/2dcnn-textclassifier\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "\n",
    "filter_sizes = [1,2,3,5]\n",
    "num_filters = 36\n",
    "\n",
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "x = Reshape((maxlen, embed_size, 1))(x)\n",
    "\n",
    "maxpool_pool = []\n",
    "for i in range(len(filter_sizes)):\n",
    "    conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),\n",
    "                                 kernel_initializer='he_normal', activation='elu')(x)\n",
    "    maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))\n",
    "\n",
    "z = Concatenate(axis=1)(maxpool_pool)   \n",
    "z = Flatten()(z)\n",
    "z = Dropout(0.1)(z)\n",
    "\n",
    "outp = Dense(1, activation=\"sigmoid\")(z)\n",
    "\n",
    "model = Model(inputs=inp, outputs=outp)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T15:50:40.974046Z",
     "start_time": "2018-11-14T15:47:06.298698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/2\n",
      "1201632/1201632 [==============================] - 104s 87us/step - loss: 0.1137 - acc: 0.9554 - val_loss: 0.1042 - val_acc: 0.9576\n",
      "Epoch 2/2\n",
      "1201632/1201632 [==============================] - 110s 92us/step - loss: 0.0924 - acc: 0.9630 - val_loss: 0.1030 - val_acc: 0.9582\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf599d4208>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train the model \n",
    "model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T15:50:45.018146Z",
     "start_time": "2018-11-14T15:50:40.976113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104490/104490 [==============================] - 3s 33us/step\n",
      "F1 score at threshold 0.1 is 0.6145605596851772\n",
      "F1 score at threshold 0.11 is 0.623360609797108\n",
      "F1 score at threshold 0.12 is 0.6315970628728775\n",
      "F1 score at threshold 0.13 is 0.6376168224299065\n",
      "F1 score at threshold 0.14 is 0.6427804936210803\n",
      "F1 score at threshold 0.15 is 0.6471730162581898\n",
      "F1 score at threshold 0.16 is 0.6512431365290888\n",
      "F1 score at threshold 0.17 is 0.6555032925682032\n",
      "F1 score at threshold 0.18 is 0.6583518930957682\n",
      "F1 score at threshold 0.19 is 0.6612392582541835\n",
      "F1 score at threshold 0.2 is 0.6644392033542977\n",
      "F1 score at threshold 0.21 is 0.6650733585607117\n",
      "F1 score at threshold 0.22 is 0.6649636999193331\n",
      "F1 score at threshold 0.23 is 0.6671652954375468\n",
      "F1 score at threshold 0.24 is 0.6686370197264416\n",
      "F1 score at threshold 0.25 is 0.6696757169640997\n",
      "F1 score at threshold 0.26 is 0.669570103961787\n",
      "F1 score at threshold 0.27 is 0.6698360888384304\n",
      "F1 score at threshold 0.28 is 0.6713456993482776\n",
      "F1 score at threshold 0.29 is 0.6712487339024743\n",
      "F1 score at threshold 0.3 is 0.6706639368236327\n",
      "F1 score at threshold 0.31 is 0.6692756405523149\n",
      "F1 score at threshold 0.32 is 0.6682075753056964\n",
      "F1 score at threshold 0.33 is 0.6667670531546454\n",
      "F1 score at threshold 0.34 is 0.666312191416635\n",
      "F1 score at threshold 0.35 is 0.6646761598530087\n",
      "F1 score at threshold 0.36 is 0.6631106988249846\n",
      "F1 score at threshold 0.37 is 0.6613632818593043\n",
      "F1 score at threshold 0.38 is 0.659535981124656\n",
      "F1 score at threshold 0.39 is 0.6563590231525531\n",
      "F1 score at threshold 0.4 is 0.6522643622979677\n",
      "F1 score at threshold 0.41 is 0.6510240283825189\n",
      "F1 score at threshold 0.42 is 0.6483936559577064\n",
      "F1 score at threshold 0.43 is 0.6461235862973282\n",
      "F1 score at threshold 0.44 is 0.6430342092216162\n",
      "F1 score at threshold 0.45 is 0.6401600400100025\n",
      "F1 score at threshold 0.46 is 0.6358362061716976\n",
      "F1 score at threshold 0.47 is 0.6327568667344862\n",
      "F1 score at threshold 0.48 is 0.6286007351055646\n",
      "F1 score at threshold 0.49 is 0.6237820125894629\n",
      "F1 score at threshold 0.5 is 0.6196324361989374\n"
     ]
    }
   ],
   "source": [
    "pred_cnn_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_cnn_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T15:50:46.795408Z",
     "start_time": "2018-11-14T15:50:45.019860Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56370/56370 [==============================] - 2s 31us/step\n"
     ]
    }
   ],
   "source": [
    "pred_cnn_test_y = model.predict([test_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T15:50:57.659941Z",
     "start_time": "2018-11-14T15:50:46.798053Z"
    }
   },
   "outputs": [],
   "source": [
    "del word_index, embeddings_index, all_embs, embedding_matrix, model, inp, x\n",
    "import gc; gc.collect()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T15:50:57.680497Z",
     "start_time": "2018-11-14T15:50:57.672751Z"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T15:53:13.588970Z",
     "start_time": "2018-11-14T15:50:57.681749Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'glove.840B.300d/glove.840B.300d.txt'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "x = Attention(maxlen)(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T15:58:28.348547Z",
     "start_time": "2018-11-14T15:53:13.590400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/3\n",
      "1201632/1201632 [==============================] - 112s 93us/step - loss: 0.1164 - acc: 0.9545 - val_loss: 0.1051 - val_acc: 0.9578\n",
      "Epoch 2/3\n",
      "1201632/1201632 [==============================] - 101s 84us/step - loss: 0.1014 - acc: 0.9596 - val_loss: 0.0997 - val_acc: 0.9600\n",
      "Epoch 3/3\n",
      "1201632/1201632 [==============================] - 101s 84us/step - loss: 0.0949 - acc: 0.9620 - val_loss: 0.0986 - val_acc: 0.9611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf3ea3d2b0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y, batch_size=512, epochs=3, validation_data=(val_X, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T15:58:31.970055Z",
     "start_time": "2018-11-14T15:58:28.349919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104490/104490 [==============================] - 3s 30us/step\n",
      "F1 score at threshold 0.1 is 0.633719288336628\n",
      "F1 score at threshold 0.11 is 0.6406447898675878\n",
      "F1 score at threshold 0.12 is 0.6465759849906191\n",
      "F1 score at threshold 0.13 is 0.6515982824427481\n",
      "F1 score at threshold 0.14 is 0.6555454820187367\n",
      "F1 score at threshold 0.15 is 0.6600942991855979\n",
      "F1 score at threshold 0.16 is 0.6633215635259865\n",
      "F1 score at threshold 0.17 is 0.6670430309873291\n",
      "F1 score at threshold 0.18 is 0.6709382731713507\n",
      "F1 score at threshold 0.19 is 0.6733371817073952\n",
      "F1 score at threshold 0.2 is 0.6755267423014588\n",
      "F1 score at threshold 0.21 is 0.6773686279648801\n",
      "F1 score at threshold 0.22 is 0.6788736118455844\n",
      "F1 score at threshold 0.23 is 0.6800933644548183\n",
      "F1 score at threshold 0.24 is 0.682454251883746\n",
      "F1 score at threshold 0.25 is 0.682447097124254\n",
      "F1 score at threshold 0.26 is 0.6828266228430567\n",
      "F1 score at threshold 0.27 is 0.6841124076503486\n",
      "F1 score at threshold 0.28 is 0.6839796699853791\n",
      "F1 score at threshold 0.29 is 0.6852137351086195\n",
      "F1 score at threshold 0.3 is 0.6860251519005228\n",
      "F1 score at threshold 0.31 is 0.6861584539684802\n",
      "F1 score at threshold 0.32 is 0.6864083950262344\n",
      "F1 score at threshold 0.33 is 0.6874230685685324\n",
      "F1 score at threshold 0.34 is 0.6861643935526219\n",
      "F1 score at threshold 0.35 is 0.684891240446796\n",
      "F1 score at threshold 0.36 is 0.684783413550537\n",
      "F1 score at threshold 0.37 is 0.6836065573770492\n",
      "F1 score at threshold 0.38 is 0.6829121991452352\n",
      "F1 score at threshold 0.39 is 0.682897323453803\n",
      "F1 score at threshold 0.4 is 0.6814893779029925\n",
      "F1 score at threshold 0.41 is 0.6797114794352364\n",
      "F1 score at threshold 0.42 is 0.6784583301150846\n",
      "F1 score at threshold 0.43 is 0.678490859587709\n",
      "F1 score at threshold 0.44 is 0.67679774400752\n",
      "F1 score at threshold 0.45 is 0.6757673794681607\n",
      "F1 score at threshold 0.46 is 0.6736122156831558\n",
      "F1 score at threshold 0.47 is 0.6725933045010412\n",
      "F1 score at threshold 0.48 is 0.67102607572455\n",
      "F1 score at threshold 0.49 is 0.6680781758957655\n",
      "F1 score at threshold 0.5 is 0.6662288622557873\n"
     ]
    }
   ],
   "source": [
    "pred_glove_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_glove_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T15:58:33.475524Z",
     "start_time": "2018-11-14T15:58:31.971372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56370/56370 [==============================] - 2s 27us/step\n"
     ]
    }
   ],
   "source": [
    "pred_glove_test_y = model.predict([test_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T15:58:44.206324Z",
     "start_time": "2018-11-14T15:58:33.476942Z"
    }
   },
   "outputs": [],
   "source": [
    "del word_index, embeddings_index, all_embs, embedding_matrix, model, inp, x\n",
    "import gc; gc.collect()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T16:04:50.448491Z",
     "start_time": "2018-11-14T15:58:44.505500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/3\n",
      "1201632/1201632 [==============================] - 102s 85us/step - loss: 0.1252 - acc: 0.9520 - val_loss: 0.1113 - val_acc: 0.9559\n",
      "Epoch 2/3\n",
      "1201632/1201632 [==============================] - 101s 84us/step - loss: 0.1084 - acc: 0.9572 - val_loss: 0.1070 - val_acc: 0.9571\n",
      "Epoch 3/3\n",
      "1201632/1201632 [==============================] - 101s 84us/step - loss: 0.1032 - acc: 0.9592 - val_loss: 0.1028 - val_acc: 0.9590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf4c848a90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_FILE = 'wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "x = Attention(maxlen)(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3), metrics=['accuracy'])\n",
    "model.fit(train_X, train_y, batch_size=512, epochs=3, validation_data=(val_X, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T16:04:54.165809Z",
     "start_time": "2018-11-14T16:04:50.449971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104490/104490 [==============================] - 3s 30us/step\n",
      "F1 score at threshold 0.1 is 0.579749509582013\n",
      "F1 score at threshold 0.11 is 0.5898812596799173\n",
      "F1 score at threshold 0.12 is 0.599217676287134\n",
      "F1 score at threshold 0.13 is 0.6070097748015337\n",
      "F1 score at threshold 0.14 is 0.6139365918097754\n",
      "F1 score at threshold 0.15 is 0.619479937233804\n",
      "F1 score at threshold 0.16 is 0.626599634369287\n",
      "F1 score at threshold 0.17 is 0.6319517009172182\n",
      "F1 score at threshold 0.18 is 0.6366961651917403\n",
      "F1 score at threshold 0.19 is 0.6409641443818204\n",
      "F1 score at threshold 0.2 is 0.645546484708176\n",
      "F1 score at threshold 0.21 is 0.648191581286261\n",
      "F1 score at threshold 0.22 is 0.6509475264244169\n",
      "F1 score at threshold 0.23 is 0.6537142857142857\n",
      "F1 score at threshold 0.24 is 0.655008357978655\n",
      "F1 score at threshold 0.25 is 0.6578947368421053\n",
      "F1 score at threshold 0.26 is 0.6584530241138489\n",
      "F1 score at threshold 0.27 is 0.6598267821452365\n",
      "F1 score at threshold 0.28 is 0.6612696485191931\n",
      "F1 score at threshold 0.29 is 0.6629834254143647\n",
      "F1 score at threshold 0.3 is 0.6632181529760672\n",
      "F1 score at threshold 0.31 is 0.6634568418114577\n",
      "F1 score at threshold 0.32 is 0.6637517630465443\n",
      "F1 score at threshold 0.33 is 0.6649087799315849\n",
      "F1 score at threshold 0.34 is 0.6649866801065591\n",
      "F1 score at threshold 0.35 is 0.6656972517085938\n",
      "F1 score at threshold 0.36 is 0.6651995305164319\n",
      "F1 score at threshold 0.37 is 0.6642470928079403\n",
      "F1 score at threshold 0.38 is 0.6638724646358806\n",
      "F1 score at threshold 0.39 is 0.6634935025687518\n",
      "F1 score at threshold 0.4 is 0.6627471940138963\n",
      "F1 score at threshold 0.41 is 0.6625539790252931\n",
      "F1 score at threshold 0.42 is 0.6599735387967935\n",
      "F1 score at threshold 0.43 is 0.6565990246971842\n",
      "F1 score at threshold 0.44 is 0.6558106283263165\n",
      "F1 score at threshold 0.45 is 0.6529204107830553\n",
      "F1 score at threshold 0.46 is 0.6506766064338384\n",
      "F1 score at threshold 0.47 is 0.6472322305928595\n",
      "F1 score at threshold 0.48 is 0.645364238410596\n",
      "F1 score at threshold 0.49 is 0.6429585107271056\n",
      "F1 score at threshold 0.5 is 0.6386355960824047\n"
     ]
    }
   ],
   "source": [
    "pred_fasttext_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_fasttext_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T16:04:55.698374Z",
     "start_time": "2018-11-14T16:04:54.167576Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56370/56370 [==============================] - 2s 27us/step\n"
     ]
    }
   ],
   "source": [
    "pred_fasttext_test_y = model.predict([test_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T16:05:06.131625Z",
     "start_time": "2018-11-14T16:04:55.700129Z"
    }
   },
   "outputs": [],
   "source": [
    "del word_index, embeddings_index, all_embs, embedding_matrix, model, inp, x\n",
    "import gc; gc.collect()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T16:06:55.226000Z",
     "start_time": "2018-11-14T16:05:06.293793Z"
    }
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'paragram_300_sl999/paragram_300_sl999.txt'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "x = Bidirectional(CuDNNLSTM(128, return_sequences=True))(x)\n",
    "x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "x = Attention(maxlen)(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T16:12:01.176265Z",
     "start_time": "2018-11-14T16:06:55.227355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/3\n",
      "1201632/1201632 [==============================] - 103s 86us/step - loss: 0.1189 - acc: 0.9542 - val_loss: 0.1055 - val_acc: 0.9575\n",
      "Epoch 2/3\n",
      "1201632/1201632 [==============================] - 101s 84us/step - loss: 0.1015 - acc: 0.9598 - val_loss: 0.1010 - val_acc: 0.9595\n",
      "Epoch 3/3\n",
      "1201632/1201632 [==============================] - 101s 84us/step - loss: 0.0934 - acc: 0.9627 - val_loss: 0.1001 - val_acc: 0.9596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf4ba3fd30>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y, batch_size=512, epochs=3, validation_data=(val_X, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T16:12:05.072985Z",
     "start_time": "2018-11-14T16:12:01.178286Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104490/104490 [==============================] - 3s 32us/step\n",
      "F1 score at threshold 0.1 is 0.5946696851203952\n",
      "F1 score at threshold 0.11 is 0.6042960935032115\n",
      "F1 score at threshold 0.12 is 0.6123985160492499\n",
      "F1 score at threshold 0.13 is 0.6189902529843391\n",
      "F1 score at threshold 0.14 is 0.6257033034371344\n",
      "F1 score at threshold 0.15 is 0.6322346811643448\n",
      "F1 score at threshold 0.16 is 0.6385973007267275\n",
      "F1 score at threshold 0.17 is 0.6444496597042948\n",
      "F1 score at threshold 0.18 is 0.6489608765557078\n",
      "F1 score at threshold 0.19 is 0.6531921331316188\n",
      "F1 score at threshold 0.2 is 0.6566542021700484\n",
      "F1 score at threshold 0.21 is 0.6591741695125738\n",
      "F1 score at threshold 0.22 is 0.6610873395419079\n",
      "F1 score at threshold 0.23 is 0.664710385302373\n",
      "F1 score at threshold 0.24 is 0.6664943457189015\n",
      "F1 score at threshold 0.25 is 0.6677126233902072\n",
      "F1 score at threshold 0.26 is 0.6683855612852042\n",
      "F1 score at threshold 0.27 is 0.6701472556894243\n",
      "F1 score at threshold 0.28 is 0.6718982546340143\n",
      "F1 score at threshold 0.29 is 0.6725458025704129\n",
      "F1 score at threshold 0.3 is 0.6714611714265957\n",
      "F1 score at threshold 0.31 is 0.6707180776753282\n",
      "F1 score at threshold 0.32 is 0.6719898247597512\n",
      "F1 score at threshold 0.33 is 0.6724224045665359\n",
      "F1 score at threshold 0.34 is 0.6725778546712803\n",
      "F1 score at threshold 0.35 is 0.6708925318761384\n",
      "F1 score at threshold 0.36 is 0.6702965197557207\n",
      "F1 score at threshold 0.37 is 0.6698365527488856\n",
      "F1 score at threshold 0.38 is 0.6688180112570357\n",
      "F1 score at threshold 0.39 is 0.6666666666666666\n",
      "F1 score at threshold 0.4 is 0.6644706603339973\n",
      "F1 score at threshold 0.41 is 0.6635239567233385\n",
      "F1 score at threshold 0.42 is 0.6628740648379052\n",
      "F1 score at threshold 0.43 is 0.6615227147468703\n",
      "F1 score at threshold 0.44 is 0.6595067621320605\n",
      "F1 score at threshold 0.45 is 0.6588972833949526\n",
      "F1 score at threshold 0.46 is 0.6566245545837383\n",
      "F1 score at threshold 0.47 is 0.6534248814778486\n",
      "F1 score at threshold 0.48 is 0.6505327496489635\n",
      "F1 score at threshold 0.49 is 0.6475156576200417\n",
      "F1 score at threshold 0.5 is 0.6440620782726045\n"
     ]
    }
   ],
   "source": [
    "pred_paragram_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_paragram_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T16:12:06.616999Z",
     "start_time": "2018-11-14T16:12:05.074536Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56370/56370 [==============================] - 2s 27us/step\n"
     ]
    }
   ],
   "source": [
    "pred_paragram_test_y = model.predict([test_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T16:12:17.261736Z",
     "start_time": "2018-11-14T16:12:06.618404Z"
    }
   },
   "outputs": [],
   "source": [
    "del word_index, embeddings_index, all_embs, embedding_matrix, model, inp, x\n",
    "import gc; gc.collect()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T03:46:19.321131Z",
     "start_time": "2018-11-15T03:45:30.856527Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert string to lower case\n",
    "train_texts = train_df['question_text'].values\n",
    "train_texts = [s.lower() for s in train_texts]\n",
    "\n",
    "val_texts = val_df['question_text'].values\n",
    "val_texts = [s.lower() for s in val_texts]\n",
    "\n",
    "# =======================Convert string to index================\n",
    "# Tokenizer\n",
    "tk = Tokenizer(num_words=None, char_level=True, oov_token='UNK')\n",
    "tk.fit_on_texts(train_texts)\n",
    "# If we already have a character list, then replace the tk.word_index\n",
    "# If not, just skip below part\n",
    "\n",
    "# -----------------------Skip part start--------------------------\n",
    "# construct a new vocabulary\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz0123456789,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"\n",
    "char_dict = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    char_dict[char] = i + 1\n",
    "\n",
    "# Use char_dict to replace the tk.word_index\n",
    "tk.word_index = char_dict.copy()\n",
    "# Add 'UNK' to the vocabulary\n",
    "tk.word_index[tk.oov_token] = max(char_dict.values()) + 1\n",
    "# -----------------------Skip part end----------------------------\n",
    "\n",
    "# Convert string to index\n",
    "train_sequences = tk.texts_to_sequences(train_texts)\n",
    "val_texts = tk.texts_to_sequences(val_texts)\n",
    "\n",
    "# Padding\n",
    "train_data = pad_sequences(train_sequences, maxlen=500, padding='post')\n",
    "val_data = pad_sequences(val_texts, maxlen=500, padding='post')\n",
    "\n",
    "# Convert to numpy array\n",
    "train_data = np.array(train_data, dtype='float32')\n",
    "val_data = np.array(val_data, dtype='float32')\n",
    "\n",
    "#=======================Get classes================\n",
    "train_classes = train_df['target'].values\n",
    "train_class_list = [x for x in train_classes]\n",
    "val_classes = val_df['target'].values\n",
    "val_class_list = [x for x in val_classes]\n",
    "# from keras.utils import to_categorical\n",
    "# train_classes = to_categorical(train_class_list)\n",
    "# val_classes = to_categorical(val_class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T03:46:19.328854Z",
     "start_time": "2018-11-15T03:46:19.327095Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tk.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T03:46:19.333177Z",
     "start_time": "2018-11-15T03:46:19.330464Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_weights =  [] #(70,69)\n",
    "embedding_weights.append(np.zeros(vocab_size))\n",
    "\n",
    "for char,i in tk.word_index.items():\n",
    "    onehot = np.zeros(vocab_size)\n",
    "    onehot[i-1] = 1\n",
    "    embedding_weights.append(onehot)\n",
    "\n",
    "embedding_weights = np.array(embedding_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T03:46:19.337843Z",
     "start_time": "2018-11-15T03:46:19.334721Z"
    }
   },
   "outputs": [],
   "source": [
    "#parameter\n",
    "input_size = 500\n",
    "embedding_size = 69\n",
    "conv_layers = [[256,7,3],\n",
    "              [256,7,3],\n",
    "              [256,3,-1],\n",
    "              [256,3,-1],\n",
    "              [256,3,-1],\n",
    "              [256,3,3]]\n",
    "\n",
    "fully_connected_layers = [1024]\n",
    "dropout_p = 0.5\n",
    "optimizer = 'adam'\n",
    "loss = 'binary_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T03:46:19.341508Z",
     "start_time": "2018-11-15T03:46:19.339416Z"
    }
   },
   "outputs": [],
   "source": [
    "#embedding layer initialization\n",
    "embedding_layer = Embedding(vocab_size+1, embedding_size, input_length=input_size, \n",
    "                            weights=[embedding_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T03:46:20.461581Z",
     "start_time": "2018-11-15T03:46:19.343387Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "embedding_7 (Embedding)      (None, 500, 69)           4830      \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 494, 256)          123904    \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (None, 494, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 164, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 158, 256)          459008    \n",
      "_________________________________________________________________\n",
      "activation_20 (Activation)   (None, 158, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 52, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 50, 256)           196864    \n",
      "_________________________________________________________________\n",
      "activation_21 (Activation)   (None, 50, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 48, 256)           196864    \n",
      "_________________________________________________________________\n",
      "activation_22 (Activation)   (None, 48, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 46, 256)           196864    \n",
      "_________________________________________________________________\n",
      "activation_23 (Activation)   (None, 46, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 44, 256)           196864    \n",
      "_________________________________________________________________\n",
      "activation_24 (Activation)   (None, 44, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 14, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 3584)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1024)              3671040   \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 1)                 1025      \n",
      "=================================================================\n",
      "Total params: 5,047,263\n",
      "Trainable params: 5,047,263\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Model\n",
    "from keras.layers import Input, Embedding, Activation, Flatten, Dense\n",
    "from keras.layers import Conv1D, MaxPooling1D, Dropout\n",
    "\n",
    "#Input\n",
    "inputs = Input(shape=(input_size,), name='input', dtype='int64')\n",
    "#Embedding\n",
    "x = embedding_layer(inputs)\n",
    "#Conv\n",
    "for filter_num, filter_size, pooling_size in conv_layers:\n",
    "    x = Conv1D(filter_num, filter_size)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    if pooling_size != -1:\n",
    "        x = MaxPooling1D(pool_size=pooling_size)(x)\n",
    "x = Flatten()(x)\n",
    "\n",
    "#FC layers\n",
    "for dense_size in fully_connected_layers:\n",
    "    x = Dense(dense_size, activation='relu')(x)\n",
    "    x = Dropout(dropout_p)(x)\n",
    "    \n",
    "#ouput\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "#build\n",
    "model = Model(inputs=inputs, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=1e-3), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T03:46:21.517023Z",
     "start_time": "2018-11-15T03:46:20.464223Z"
    }
   },
   "outputs": [],
   "source": [
    "#Shuffle\n",
    "indices = np.arange(train_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "x_train = train_data[indices]\n",
    "y_train = train_classes[indices]\n",
    "\n",
    "x_val = val_data\n",
    "y_val = val_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T04:34:13.865950Z",
     "start_time": "2018-11-15T03:46:21.518441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1201632 samples, validate on 104490 samples\n",
      "Epoch 1/10\n",
      " - 288s - loss: 0.1372 - acc: 0.9490 - val_loss: 0.1246 - val_acc: 0.9517\n",
      "Epoch 2/10\n",
      " - 287s - loss: 0.1187 - acc: 0.9538 - val_loss: 0.1174 - val_acc: 0.9532\n",
      "Epoch 3/10\n",
      " - 287s - loss: 0.1128 - acc: 0.9556 - val_loss: 0.1202 - val_acc: 0.9540\n",
      "Epoch 4/10\n",
      " - 287s - loss: 0.1078 - acc: 0.9573 - val_loss: 0.1189 - val_acc: 0.9532\n",
      "Epoch 5/10\n",
      " - 287s - loss: 0.1025 - acc: 0.9593 - val_loss: 0.1176 - val_acc: 0.9538\n",
      "Epoch 6/10\n",
      " - 287s - loss: 0.0968 - acc: 0.9616 - val_loss: 0.1201 - val_acc: 0.9544\n",
      "Epoch 7/10\n",
      " - 287s - loss: 0.0899 - acc: 0.9641 - val_loss: 0.1233 - val_acc: 0.9538\n",
      "Epoch 8/10\n",
      " - 287s - loss: 0.0833 - acc: 0.9669 - val_loss: 0.1276 - val_acc: 0.9532\n",
      "Epoch 9/10\n",
      " - 288s - loss: 0.0755 - acc: 0.9703 - val_loss: 0.1297 - val_acc: 0.9516\n",
      "Epoch 10/10\n",
      " - 286s - loss: 0.0685 - acc: 0.9731 - val_loss: 0.1452 - val_acc: 0.9524\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbf48e6acc0>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training\n",
    "model.fit(x_train, y_train,\n",
    "          validation_data=(x_val, y_val),\n",
    "          batch_size=128,\n",
    "          epochs=10,\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-15T10:17:11.716724Z",
     "start_time": "2018-11-15T10:17:05.468115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104490/104490 [==============================] - 6s 54us/step\n",
      "F1 score at threshold 0.1 is 0.5930435909061467\n",
      "F1 score at threshold 0.11 is 0.5959329195827281\n",
      "F1 score at threshold 0.12 is 0.5969346598547997\n",
      "F1 score at threshold 0.13 is 0.5978565089767218\n",
      "F1 score at threshold 0.14 is 0.5988239363542027\n",
      "F1 score at threshold 0.15 is 0.5989477376359172\n",
      "F1 score at threshold 0.16 is 0.5978284011070896\n",
      "F1 score at threshold 0.17 is 0.5987224574750591\n",
      "F1 score at threshold 0.18 is 0.5987667754805948\n",
      "F1 score at threshold 0.19 is 0.5989023051591656\n",
      "F1 score at threshold 0.2 is 0.5975330526626782\n",
      "F1 score at threshold 0.21 is 0.5970750634233696\n",
      "F1 score at threshold 0.22 is 0.5963855421686747\n",
      "F1 score at threshold 0.23 is 0.5963498098859316\n",
      "F1 score at threshold 0.24 is 0.5957055214723926\n",
      "F1 score at threshold 0.25 is 0.5953301376217721\n",
      "F1 score at threshold 0.26 is 0.5934219734079775\n",
      "F1 score at threshold 0.27 is 0.5918031502233366\n",
      "F1 score at threshold 0.28 is 0.5920896818504777\n",
      "F1 score at threshold 0.29 is 0.5913430935709739\n",
      "F1 score at threshold 0.3 is 0.5890839144025006\n",
      "F1 score at threshold 0.31 is 0.5876555178542574\n",
      "F1 score at threshold 0.32 is 0.5857829167005945\n",
      "F1 score at threshold 0.33 is 0.5837163493105713\n",
      "F1 score at threshold 0.34 is 0.5812856788284934\n",
      "F1 score at threshold 0.35 is 0.5798284334138419\n",
      "F1 score at threshold 0.36 is 0.5761689291101055\n",
      "F1 score at threshold 0.37 is 0.5746331590487436\n",
      "F1 score at threshold 0.38 is 0.5731520815632964\n",
      "F1 score at threshold 0.39 is 0.5718442447582371\n",
      "F1 score at threshold 0.4 is 0.5697074010327022\n",
      "F1 score at threshold 0.41 is 0.5680576038865274\n",
      "F1 score at threshold 0.42 is 0.5656195462478185\n",
      "F1 score at threshold 0.43 is 0.5628511235955057\n",
      "F1 score at threshold 0.44 is 0.5609885260370697\n",
      "F1 score at threshold 0.45 is 0.557639320949249\n",
      "F1 score at threshold 0.46 is 0.5546805083228924\n",
      "F1 score at threshold 0.47 is 0.5527737752161384\n",
      "F1 score at threshold 0.48 is 0.5487992750339828\n",
      "F1 score at threshold 0.49 is 0.5474585534705776\n",
      "F1 score at threshold 0.5 is 0.5439207775536402\n"
     ]
    }
   ],
   "source": [
    "pred_char_cnn_val = model.predict([x_val], batch_size=1024, verbose=1)\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(y_val, (pred_char_cnn_val>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_char_cnn_test = model.predict([test_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-14T16:13:08.516410Z",
     "start_time": "2018-11-14T15:43:58.289Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_val_y = (4 * pred_glove_val_y + pred_fasttext_val_y + 3 * pred_paragram_val_y + 2 * pred_cnn_val_y) / 10.0\n",
    "\n",
    "thresholds = []\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    res = metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\n",
    "    thresholds.append([thresh, res])\n",
    "    print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n",
    "    \n",
    "thresholds.sort(key=lambda x: x[1], reverse=True)\n",
    "best_thresh = thresholds[0][0]\n",
    "print(\"Best threshold: \", best_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:public36]",
   "language": "python",
   "name": "conda-env-public36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
